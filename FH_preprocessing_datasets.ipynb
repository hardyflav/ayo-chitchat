{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c41b0c613dee1e0682422445102e37a4770999448f85d6c79ecc4a03e8e4590"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approach = 'FH'\n",
    "path_data_samples = \"./\"+Approach+\"_data_samples/\"+Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of episodes:\n27333\nNumber of episodes in training set:\n21866\nNumber of episodes in validation set:\n2733\nNumber of episodes in test set:\n2733\n \n Number of INVALID messages:\n0\n"
     ]
    }
   ],
   "source": [
    "# Quantifying Training and Test Sets\n",
    "\n",
    "# Concatenating female and male datasets\n",
    "file_female_abs = path_data_samples + \"_output_parlai_female.txt\"\n",
    "file_male_abs = path_data_samples + \"_output_parlai_male.txt\"\n",
    "file_complete_abs = path_data_samples + \"_complete_data.txt\" # Concatenated file\n",
    "\n",
    "file_m=open(file_male_abs,\"r\")\n",
    "file_f=open(file_female_abs,\"r\")\n",
    "f = open(file_complete_abs, \"w\")\n",
    "f.write(file_m.read())\n",
    "f.write(file_f.read())\n",
    "\n",
    "# Counts number of episodes, texts and labels\n",
    "def counter(file_str):\n",
    "    file=open(file_str,\"r\")\n",
    "    nepisodes=0\n",
    "    ntexts=0\n",
    "    nlabels=0\n",
    "    for line in file:\n",
    "        #words = line.split()\n",
    "        if 'episode_done:True' in line:\n",
    "            nepisodes=nepisodes+1\n",
    "        if 'text:' in line:\n",
    "            ntexts=ntexts+1\n",
    "        if 'labels:' in line:\n",
    "            nlabels=nlabels+1\n",
    "    return nepisodes,ntexts,nlabels\n",
    "\n",
    "# Calculate number of episodes in training, validation and test set\n",
    "def split_number(file_str,f_train,f_test):\n",
    "    N=counter(file_str)[0] # Number of episodes\n",
    "    return int(N*f_train),int(N*f_test),int(N*(1.-f_train-f_test))\n",
    "\n",
    "n_train,n_valid,n_test=split_number(file_complete_abs,0.8,0.1)\n",
    "print('Total number of episodes:')\n",
    "print(counter(file_complete_abs)[0])\n",
    "print('Number of episodes in training set:')\n",
    "print(n_train)\n",
    "print('Number of episodes in validation set:')\n",
    "print(n_valid)\n",
    "print('Number of episodes in test set:')\n",
    "print(n_test)\n",
    "\n",
    "print(' \\n Number of INVALID messages:')\n",
    "print(file_complete_abs.count('[INVALID]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset (FH_complete_data -> FH_complete_data_shuffled)\n",
    "import random\n",
    "\n",
    "# def shuffle2(FilePath):\n",
    "#     with open(FilePath, mode=\"r\", encoding=\"utf-8\") as myFile:\n",
    "#         lines_list = list(myFile)\n",
    "#         episode_list=[]\n",
    "#         current_episode=[]\n",
    "#         for line in lines_list:\n",
    "#             # breakpoint()\n",
    "#             if len(line)<18 or not(line[-18:]=='episode_done:True\\n'):\n",
    "#                 current_episode.append(line)\n",
    "#             else:\n",
    "#                 # breakpoint()\n",
    "#                 current_episode.append(line)\n",
    "#                 episode_list.extend(current_episode)\n",
    "#         breakpoint()\n",
    "#     return random.sample(episode_list, len(episode_list))\n",
    "\n",
    "def shuffle2(FilePath):\n",
    "    with open(FilePath, mode=\"r\", encoding=\"utf-8\") as myFile:\n",
    "        EpisodeList = myFile.read().split('episode_done:True')\n",
    "        EpisodeList_dones = []\n",
    "        for episode in EpisodeList:\n",
    "            EpisodeList_dones.append(episode + 'episode_done:True')\n",
    "    return random.sample(EpisodeList_dones, len(EpisodeList_dones))\n",
    "\n",
    "file_complete_abs_shuffled = path_data_samples + \"_complete_data_shuffled.txt\"\n",
    "with open(file_complete_abs_shuffled, 'w') as file:\n",
    "    for item in shuffle2(file_complete_abs):\n",
    "        file.write(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Concatenating, shuffling, splitting datasets... done\n"
     ]
    }
   ],
   "source": [
    "# Splitting dasets into training, validation and test sets\n",
    "\n",
    "data_train_txt = open(path_data_samples + \"_data_train.txt\", \"w\")\n",
    "data_valid_txt = open(path_data_samples + \"_data_valid.txt\", \"w\")\n",
    "data_test_txt = open(path_data_samples + \"_data_test.txt\", \"w\")\n",
    "n_train,n_valid,n_test=split_number(file_complete_abs_shuffled,0.8,0.1)\n",
    "\n",
    "# Opening shuffled complete data set\n",
    "file=open(file_complete_abs_shuffled,\"r\")\n",
    "c_train=0\n",
    "c_test=0\n",
    "c_valid=0\n",
    "\n",
    "for line in file:\n",
    "    c_train=c_train+1\n",
    "    c_test=c_test+1\n",
    "    c_valid=c_valid+1\n",
    "    if c_train<=n_train:\n",
    "        data_train_txt.write(line)\n",
    "    if c_test>n_train and c_test<=n_train+n_test:\n",
    "        data_test_txt.write(line)\n",
    "    if c_valid>n_train+n_test and c_valid<=n_train+n_test+n_valid:\n",
    "        data_valid_txt.write(line)\n",
    "    \n",
    "data_train_txt.close()\n",
    "data_test_txt.close()\n",
    "data_valid_txt.close()\n",
    "\n",
    "print('Concatenating, shuffling, splitting datasets... done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}